{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf23662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb9755",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f506e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'    # * 를 이용해 전체 파일을 불러온다.\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []   # 불러온 텍스트 파일을 담는 리스트\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66ccf",
   "metadata": {},
   "source": [
    "# 2. 문장 중에 대화가 있는 문장만 골라내기(Tensorflow의 데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54f46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0 : continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 2: break   # 문장 3개 확인해보기\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ccb74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 5 .. 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence\n",
    "\n",
    "# 처리된 문장 확인하기\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec388e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추출한 문장을 담는 리스트\n",
    "corpus = []\n",
    "\n",
    "#  원하는 문장만 추출\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 결과물을 3개 확인해 본다\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bad949",
   "metadata": {},
   "source": [
    "## 토큰화 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba6128",
   "metadata": {},
   "source": [
    "[Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d08c4",
   "metadata": {},
   "source": [
    "### maxlen을 이용하면 시퀸스의 최대길이를 정할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fd5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1013    3]\n",
      " [  37   15 9049 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f8a025f4970>\n",
      "[[   2   50    5   91  297   65   57    9  969 6042]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329]\n",
      " [   2   36    7   37   15  164  282   28  299    4]]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  #12000개의 단어장 \n",
    "        filters=' ',     # 데이터를 정제 했기때문에 사용할 필요가 없다\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus 함수를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15) \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "print(tensor[:3, :10])     # 문장 뒤에 padding이 붙어서 0이 출력되는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b8bf12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:       #tokenizer에 구축된 단어 사전의 인덱스\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fe87b",
   "metadata": {},
   "source": [
    "### padding이란 데이터의 크기를 맞춰주기 위해 특정 값을 채워넣는 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5b4223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다, padding 값은 의미가 없으므로 제거한다.\n",
    "src_input = tensor[:, :-1]   # 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다. 출력값을 보면 끝 부분이 0인 것이 padding\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    # 앞에서부터 진행한다. start를 잘라준다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c2938",
   "metadata": {},
   "source": [
    "# 3. 데이터를 훈련데이터와 평가 데이터(전체의 20%)로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbd7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input, \n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=42,) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b86945",
   "metadata": {},
   "source": [
    "# 4. 데이터 셋 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c45e731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d1e3e",
   "metadata": {},
   "source": [
    "# 5. 작사 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e71ce7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256        # 배치 사이즈\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cfb62",
   "metadata": {},
   "source": [
    "embedding_size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기할 수 있다.\n",
    "hidden_size 값은 '판단하는 사람의 수'라고 생각할 수 있다. 충분한 데이터가 있으면 상관이 없지만 적은 데이터라면 오히려 혼란을 일으킬 수 있다. 사공이 많으면 배가 산으로 간다는 말이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ff3b545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.93798100e-04, -1.72024054e-04, -1.46827748e-04, ...,\n",
       "          1.28147862e-04, -6.04810120e-05, -5.16454566e-05],\n",
       "        [ 2.01396499e-04, -2.36089054e-05, -2.08698359e-04, ...,\n",
       "         -1.68045546e-04, -3.44414468e-04, -2.25933600e-05],\n",
       "        [ 5.15121152e-04,  1.05894607e-04, -3.80054029e-04, ...,\n",
       "         -3.84451239e-04, -4.61123651e-04, -2.69637181e-04],\n",
       "        ...,\n",
       "        [ 7.04345643e-04, -4.48004248e-05,  1.10215165e-04, ...,\n",
       "         -5.90928306e-04,  6.07105205e-04, -3.47102068e-05],\n",
       "        [ 4.20611410e-04, -1.07341912e-04,  3.57492099e-04, ...,\n",
       "         -6.54887990e-04,  6.55719719e-04,  1.94418117e-05],\n",
       "        [ 4.01993282e-04, -2.35318294e-04,  3.12771124e-04, ...,\n",
       "         -7.49046798e-04,  6.39905687e-04,  2.62548652e-04]],\n",
       "\n",
       "       [[ 1.93798100e-04, -1.72024054e-04, -1.46827748e-04, ...,\n",
       "          1.28147862e-04, -6.04810120e-05, -5.16454566e-05],\n",
       "        [ 1.40462216e-04, -4.70391649e-04, -2.86527589e-04, ...,\n",
       "          3.31399409e-04,  1.45687562e-04, -1.43224868e-04],\n",
       "        [ 4.43514436e-04, -4.70264844e-04, -3.01876105e-04, ...,\n",
       "          5.35783533e-04,  3.82656057e-04, -2.52918340e-04],\n",
       "        ...,\n",
       "        [ 2.95813050e-04,  5.21350186e-04,  3.57531040e-04, ...,\n",
       "          8.38987035e-05,  3.40187617e-05, -4.13362897e-04],\n",
       "        [ 4.70209779e-04,  3.91610374e-04,  3.28918657e-04, ...,\n",
       "         -2.96060403e-04,  1.69245002e-04, -3.08787625e-04],\n",
       "        [ 6.62101957e-04,  2.67825380e-04,  3.36923316e-04, ...,\n",
       "         -6.70753885e-04,  3.31964868e-04, -1.44827703e-04]],\n",
       "\n",
       "       [[ 1.93798100e-04, -1.72024054e-04, -1.46827748e-04, ...,\n",
       "          1.28147862e-04, -6.04810120e-05, -5.16454566e-05],\n",
       "        [ 2.98809726e-04, -2.37663029e-04, -4.61193995e-04, ...,\n",
       "          3.86995453e-05, -1.28025829e-04, -1.28117381e-05],\n",
       "        [ 3.72541748e-04, -5.62824571e-05, -4.83517186e-04, ...,\n",
       "          1.10668392e-04, -2.22605013e-05,  5.28937089e-05],\n",
       "        ...,\n",
       "        [ 1.11899467e-03, -2.82838446e-04, -2.03033225e-04, ...,\n",
       "         -1.45878654e-03,  1.14130054e-03,  7.08005682e-04],\n",
       "        [ 9.89365508e-04, -3.56057280e-04, -2.92029668e-04, ...,\n",
       "         -1.60066097e-03,  1.07083889e-03,  7.52339198e-04],\n",
       "        [ 8.36121617e-04, -4.44979261e-04, -3.97081603e-04, ...,\n",
       "         -1.69789325e-03,  9.79926670e-04,  7.78943591e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.26247745e-04,  8.82951426e-05,  3.11665681e-05, ...,\n",
       "          5.00687929e-05,  1.81448140e-05, -1.05310544e-04],\n",
       "        [ 6.74649418e-05,  3.70281487e-05, -1.49478699e-04, ...,\n",
       "          8.13306979e-05, -9.90871995e-05, -1.69295759e-04],\n",
       "        [-8.61722219e-05,  1.94263819e-04, -1.27354127e-04, ...,\n",
       "          5.60742046e-05,  1.13652059e-05, -2.13371110e-04],\n",
       "        ...,\n",
       "        [ 8.46192474e-04,  6.31469768e-04,  5.42466820e-04, ...,\n",
       "          3.35717391e-06, -2.83838512e-04, -8.17370950e-04],\n",
       "        [ 7.67456368e-04,  6.75366726e-04,  4.75460402e-04, ...,\n",
       "          1.24321188e-04, -1.84521516e-04, -8.32244114e-04],\n",
       "        [ 5.98552637e-04,  6.91945606e-04,  2.53440026e-04, ...,\n",
       "          1.30599161e-04, -1.52188237e-04, -6.97717885e-04]],\n",
       "\n",
       "       [[ 1.93798100e-04, -1.72024054e-04, -1.46827748e-04, ...,\n",
       "          1.28147862e-04, -6.04810120e-05, -5.16454566e-05],\n",
       "        [ 2.18005443e-04, -2.62599351e-04, -3.45205743e-04, ...,\n",
       "          3.59464102e-05, -2.78357493e-05,  4.65129669e-05],\n",
       "        [-1.17598211e-05, -3.02852714e-04, -3.88743181e-04, ...,\n",
       "          2.76060775e-04,  1.63209392e-04,  1.18485506e-04],\n",
       "        ...,\n",
       "        [ 3.50406684e-04, -6.09528761e-05, -1.81742682e-04, ...,\n",
       "         -9.85919614e-04,  9.33066593e-04, -2.58447340e-04],\n",
       "        [ 3.95215844e-04, -1.22654252e-04, -2.81807268e-04, ...,\n",
       "         -1.22178684e-03,  9.63992323e-04, -1.78657661e-04],\n",
       "        [ 3.99713143e-04, -2.08123587e-04, -3.99156648e-04, ...,\n",
       "         -1.39796711e-03,  9.56669333e-04, -1.02436577e-04]],\n",
       "\n",
       "       [[ 1.93798100e-04, -1.72024054e-04, -1.46827748e-04, ...,\n",
       "          1.28147862e-04, -6.04810120e-05, -5.16454566e-05],\n",
       "        [ 1.31785506e-04, -2.12464714e-04, -2.67667521e-04, ...,\n",
       "         -2.02277020e-04,  1.47905041e-04,  1.34137836e-05],\n",
       "        [-6.22733808e-07, -2.60112400e-04, -2.26926917e-04, ...,\n",
       "         -4.05304891e-04,  2.57075153e-04, -1.03507802e-04],\n",
       "        ...,\n",
       "        [ 2.15559921e-04, -3.76568263e-04,  6.63095037e-04, ...,\n",
       "         -1.23309565e-03,  9.40187485e-04,  3.09767202e-04],\n",
       "        [ 2.83855130e-04, -3.80475132e-04,  5.73755067e-04, ...,\n",
       "         -1.45031069e-03,  9.61419137e-04,  4.33920213e-04],\n",
       "        [ 3.02464032e-04, -4.14445210e-04,  4.42425662e-04, ...,\n",
       "         -1.62591285e-03,  9.47768742e-04,  5.25960349e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러온다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "# 테스트 용으로 한 배치만 불러온 데이터를 모델에 넣어보았다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4464fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()   #모델의 구조 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07fd0e",
   "metadata": {},
   "source": [
    "# 6. 손실함수 loss 구해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120453ce",
   "metadata": {},
   "source": [
    "## [loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2df78bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 112s 160ms/step - loss: 3.6060\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 3.1202\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.9165\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.7614\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.6259\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.5022\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.3869\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 112s 163ms/step - loss: 2.2799\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 112s 164ms/step - loss: 2.1786\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 112s 164ms/step - loss: 2.0819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89ec421e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a639f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6768b915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she s got me runnin round and round <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee423715",
   "metadata": {},
   "source": [
    "# 7. 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1ec8a",
   "metadata": {},
   "source": [
    "'she s got me runnin round and round' , 그녀는 나를 빙빙 돌게 했다. epochs=10을 했을때 내가 얻은 답변이다. loss는 2.1464 였고 'she s'(주어)->got(동사)->me(대명사)(목적어)->round and round(부사) 순서로 그럴듯한 문장이 완성되는 것을 확인할 수 있었다. 순환 신경망 모델이라서 교육하는데 시간이 좀 걸리는데다가 데이터까지 많아서 한번 테스트하는데 15분정도 걸렸던 것 같다. 하지만 epochs 값을 늘리면 좀더 좋은 문구를 얻지 않을까 라는 생각에 epochs를 20까지 늘리고 진행해 보았다. loss는 1.4998 가 나왔지만 문장은 같은 문장인 'she s got me runnin round and round' 를 받았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df6b61",
   "metadata": {},
   "source": [
    "epochs 값이 문장을 구성하는데 영향을 주지 못하는 것 같아서 5로 주고 실험을 진행했다. 'she s got a good girl' 라는 문구를 얻었다. 그녀는 좋은 소녀를 얻었다 . ???? 문장이 이상한 것을 확인할 수 있었다. 결론은 loss값이 클수록 이상한 말을 작사하고 loss값이 2.2 아래로는 어느정도 말이되는 문장을 완성한다는 것을 알게 되었다. LSTM 모델을 사용해서 쓰래기 값을 버리기 때문에 그럴듯한 문장이 나온 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
