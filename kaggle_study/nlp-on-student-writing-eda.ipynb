{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize - Evaluating Student Writing\n\nGeorgia State University (GSU) is an undergraduate and graduate urban public research institution in Atlanta. U.S. News & World Report ranked GSU as one of the most innovative universities in the nation. GSU awards more bachelor’s degrees to African-Americans than any other non-profit college or university in the country. GSU and The Learning Agency Lab, an independent nonprofit based in Arizona, are focused on developing science of learning-based tools and programs for social good.\n\nIn this competition, you’ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.\n\nIf successful, you'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.","metadata":{"papermill":{"duration":0.032125,"end_time":"2022-01-08T11:23:30.419348","exception":false,"start_time":"2022-01-08T11:23:30.387223","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":10.822966,"end_time":"2022-01-08T11:23:41.274393","exception":false,"start_time":"2022-01-08T11:23:30.451427","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:14.039860Z","iopub.execute_input":"2022-02-06T11:54:14.040246Z","iopub.status.idle":"2022-02-06T11:54:24.200373Z","shell.execute_reply.started":"2022-02-06T11:54:14.040137Z","shell.execute_reply":"2022-02-06T11:54:24.199412Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n\nsample_submission = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')\n\n#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\ntrain_txt = glob('../input/feedback-prize-2021/train/*.txt') \ntest_txt = glob('../input/feedback-prize-2021/test/*.txt')","metadata":{"_kg_hide-input":true,"papermill":{"duration":2.34821,"end_time":"2022-01-08T11:23:43.652956","exception":false,"start_time":"2022-01-08T11:23:41.304746","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:24.201935Z","iopub.execute_input":"2022-02-06T11:54:24.202160Z","iopub.status.idle":"2022-02-06T11:54:26.083382Z","shell.execute_reply.started":"2022-02-06T11:54:24.202134Z","shell.execute_reply":"2022-02-06T11:54:26.082277Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Introduction to the competition\n\nBasically, we have a bunch of essays written by kids in the age range of about 12-18 years old in which we have to find word sequences that can be classified as one of 7 \"discourse types\". These are:\n\n- Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis\n- Position - an opinion or conclusion on the main question\n- Claim - a claim that supports the position\n- Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n- Rebuttal - a claim that refutes a counterclaim\n- Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n- Concluding Statement - a concluding statement that restates the claims\n\nLet's look at the full text of one essay first.","metadata":{"papermill":{"duration":0.029708,"end_time":"2022-01-08T11:23:43.714365","exception":false,"start_time":"2022-01-08T11:23:43.684657","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!cat ../input/feedback-prize-2021/train/423A1CA112E2.txt","metadata":{"papermill":{"duration":0.850585,"end_time":"2022-01-08T11:23:44.594785","exception":false,"start_time":"2022-01-08T11:23:43.7442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:26.084456Z","iopub.execute_input":"2022-02-06T11:54:26.086327Z","iopub.status.idle":"2022-02-06T11:54:26.849720Z","shell.execute_reply.started":"2022-02-06T11:54:26.086294Z","shell.execute_reply":"2022-02-06T11:54:26.848812Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The train dataset gives us the following human annotations that are extracted from this essay.","metadata":{"papermill":{"duration":0.03024,"end_time":"2022-01-08T11:23:44.655836","exception":false,"start_time":"2022-01-08T11:23:44.625596","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.query('id == \"423A1CA112E2\"')","metadata":{"papermill":{"duration":0.075818,"end_time":"2022-01-08T11:23:44.762031","exception":false,"start_time":"2022-01-08T11:23:44.686213","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:26.852002Z","iopub.execute_input":"2022-02-06T11:54:26.852256Z","iopub.status.idle":"2022-02-06T11:54:26.893413Z","shell.execute_reply.started":"2022-02-06T11:54:26.852228Z","shell.execute_reply":"2022-02-06T11:54:26.892587Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Kaggle gives us the following field descriptions:\n- id - ID code for essay response\n- discourse_id - ID code for discourse element\n- discourse_start - character position where discourse element begins in the essay response\n- discourse_end - character position where discourse element ends in the essay response\n- discourse_text - text of discourse element\n- discourse_type - classification of discourse element\n- discourse_type_num - enumerated class label of discourse element\n- predictionstring - the word indices of the training sample, as required for predictions\n\nThe Ground Truth here is a combination of the discourse type and the prediction string. The predictionstring corresponds to the index of the words in the essay and the predicted discourse type for this sequence of words should be correct. There can be partial matches, if the correct discourse type is predicted but on a longer or shorter sequence of words than specified in the Ground Truth.\n\nAs we can see, not necessarily all text of an essay is part of a discourse. In this case, the title is not part of any discourse.\n\n\n# Lenght of the discourse_text and predictionstring\nFirst, I would like to check if the discourse_text and the predictionstring always have the same number of words (as they should).","metadata":{"papermill":{"duration":0.031218,"end_time":"2022-01-08T11:23:44.825988","exception":false,"start_time":"2022-01-08T11:23:44.79477","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#add columns\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\n\ncols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\ntrain[cols_to_display].head()","metadata":{"papermill":{"duration":0.771051,"end_time":"2022-01-08T11:23:45.628181","exception":false,"start_time":"2022-01-08T11:23:44.85713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:26.894581Z","iopub.execute_input":"2022-02-06T11:54:26.894814Z","iopub.status.idle":"2022-02-06T11:54:27.817762Z","shell.execute_reply.started":"2022-02-06T11:54:26.894787Z","shell.execute_reply":"2022-02-06T11:54:27.816884Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Is this always correct? No, I find 468 discourses where this goes wrong (by one word)","metadata":{"papermill":{"duration":0.032411,"end_time":"2022-01-08T11:23:45.692707","exception":false,"start_time":"2022-01-08T11:23:45.660296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"The total number of discourses is {len(train)}\")\ntrain.query('discourse_len != pred_len')[cols_to_display]","metadata":{"papermill":{"duration":0.054256,"end_time":"2022-01-08T11:23:45.777965","exception":false,"start_time":"2022-01-08T11:23:45.723709","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:27.819040Z","iopub.execute_input":"2022-02-06T11:54:27.819313Z","iopub.status.idle":"2022-02-06T11:54:27.843619Z","shell.execute_reply.started":"2022-02-06T11:54:27.819285Z","shell.execute_reply":"2022-02-06T11:54:27.842778Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Let's check the first one.","metadata":{"papermill":{"duration":0.032999,"end_time":"2022-01-08T11:23:45.843368","exception":false,"start_time":"2022-01-08T11:23:45.810369","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(train.query('discourse_id == 1622473475289')['discourse_text'].values[0])\nprint(train.query('discourse_id == 1622473475289')['discourse_text'].values[0].split())\nprint(len(train.query('discourse_id == 1622473475289')['discourse_text'].values[0].split()))","metadata":{"papermill":{"duration":0.051374,"end_time":"2022-01-08T11:23:45.928099","exception":false,"start_time":"2022-01-08T11:23:45.876725","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:27.845997Z","iopub.execute_input":"2022-02-06T11:54:27.846299Z","iopub.status.idle":"2022-02-06T11:54:27.866021Z","shell.execute_reply.started":"2022-02-06T11:54:27.846261Z","shell.execute_reply":"2022-02-06T11:54:27.865239Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The length of 19 words seems correct to me, and the length of the predictionstring also really seems to be 18. Something to keep in mind.","metadata":{"papermill":{"duration":0.032337,"end_time":"2022-01-08T11:23:45.99318","exception":false,"start_time":"2022-01-08T11:23:45.960843","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(train.query('discourse_id == 1622473475289')['predictionstring'].values[0])\nprint(train.query('discourse_id == 1622473475289')['predictionstring'].values[0].split())\nprint(len(train.query('discourse_id == 1622473475289')['predictionstring'].values[0].split()))","metadata":{"papermill":{"duration":0.052451,"end_time":"2022-01-08T11:23:46.078066","exception":false,"start_time":"2022-01-08T11:23:46.025615","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:27.866936Z","iopub.execute_input":"2022-02-06T11:54:27.867464Z","iopub.status.idle":"2022-02-06T11:54:27.884320Z","shell.execute_reply.started":"2022-02-06T11:54:27.867429Z","shell.execute_reply":"2022-02-06T11:54:27.883290Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Length and frequency and relative position per discourse_type\n\nIs there a correlation between the length of a discourse and the class (discourse_type)? Yes, there is. Evidence is the longest discount type on average. When looking at the frequencies of occurence, we see that Counterclaim and Rebuttal are relatively rare","metadata":{"papermill":{"duration":0.033815,"end_time":"2022-01-08T11:23:46.147492","exception":false,"start_time":"2022-01-08T11:23:46.113677","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\nax1 = fig.add_subplot(211)\nax1 = train.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind=\"barh\")\nax1.set_title(\"Average number of words versus Discourse Type\", fontsize=14, fontweight = 'bold')\nax1.set_xlabel(\"Average number of words\", fontsize = 10)\nax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(212)\nax2 = train.groupby('discourse_type')['discourse_type'].count().sort_values().plot(kind=\"barh\")\nax2.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ','))) #add thousands separator\nax2.set_title(\"Frequency of Discourse Type in all essays\", fontsize=14, fontweight = 'bold')\nax2.set_xlabel(\"Frequency\", fontsize = 10)\nax2.set_ylabel(\"\")\n\nplt.tight_layout(pad=2)\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.553533,"end_time":"2022-01-08T11:23:46.733922","exception":false,"start_time":"2022-01-08T11:23:46.180389","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:27.885689Z","iopub.execute_input":"2022-02-06T11:54:27.885913Z","iopub.status.idle":"2022-02-06T11:54:28.422142Z","shell.execute_reply.started":"2022-02-06T11:54:27.885887Z","shell.execute_reply":"2022-02-06T11:54:28.421306Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We do have the field discourse_type_num. We see that Evidence1, Position1 and Claim1 are almost always there in an essay. Most students also had at least one Concluding Statement. What's surprising to me is that a Lead is missing in about 40% of the essays (Lead 1 is found in almost 60% of the essays).\n\nThe graph only plots discourse_type_nums which are found in at least 3% of the essays.","metadata":{"papermill":{"duration":0.034765,"end_time":"2022-01-08T11:23:46.804525","exception":false,"start_time":"2022-01-08T11:23:46.76976","status":"completed"},"tags":[]}},{"cell_type":"code","source":"av_per_essay = train['discourse_type_num'].value_counts(ascending = True).rename_axis('discourse_type_num').reset_index(name='count')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.424924,"end_time":"2022-01-08T11:23:47.263648","exception":false,"start_time":"2022-01-08T11:23:46.838724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:28.425491Z","iopub.execute_input":"2022-02-06T11:54:28.426135Z","iopub.status.idle":"2022-02-06T11:54:28.452568Z","shell.execute_reply.started":"2022-02-06T11:54:28.426094Z","shell.execute_reply":"2022-02-06T11:54:28.451624Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(av_per_essay)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:28.454180Z","iopub.execute_input":"2022-02-06T11:54:28.454488Z","iopub.status.idle":"2022-02-06T11:54:28.466798Z","shell.execute_reply.started":"2022-02-06T11:54:28.454444Z","shell.execute_reply":"2022-02-06T11:54:28.465981Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nav_per_essay = train['discourse_type_num'].value_counts(ascending = True).rename_axis('discourse_type_num').reset_index(name='count')\nav_per_essay['perc'] = round((av_per_essay['count'] / train.id.nunique()),3)\nav_per_essay = av_per_essay.set_index('discourse_type_num')\nax = av_per_essay.query('perc > 0.03')['perc'].plot(kind=\"barh\")\nax.set_title(\"discourse_type_num: Percent present in essays\", fontsize=20, fontweight = 'bold')\nax.bar_label(ax.containers[0], label_type=\"edge\")\nax.set_xlabel(\"Percent\")\nax.set_ylabel(\"\")\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.424924,"end_time":"2022-01-08T11:23:47.263648","exception":false,"start_time":"2022-01-08T11:23:46.838724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:28.470020Z","iopub.execute_input":"2022-02-06T11:54:28.470310Z","iopub.status.idle":"2022-02-06T11:54:28.916772Z","shell.execute_reply.started":"2022-02-06T11:54:28.470281Z","shell.execute_reply":"2022-02-06T11:54:28.916097Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(av_per_essay)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:54:28.917953Z","iopub.execute_input":"2022-02-06T11:54:28.918281Z","iopub.status.idle":"2022-02-06T11:54:28.927253Z","shell.execute_reply.started":"2022-02-06T11:54:28.918254Z","shell.execute_reply":"2022-02-06T11:54:28.926529Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Below you can see a plot with the average positions of the discourse start and end.","metadata":{"papermill":{"duration":0.036282,"end_time":"2022-01-08T11:23:47.336531","exception":false,"start_time":"2022-01-08T11:23:47.300249","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = train.groupby(\"discourse_type\")[['discourse_end', 'discourse_start']].mean().reset_index().sort_values(by = 'discourse_start', ascending = False)\ndata.plot(x='discourse_type',\n        kind='barh',\n        stacked=False,\n        title='Average start and end position absolute',\n        figsize=(12,4))\nplt.show()","metadata":{"papermill":{"duration":0.305013,"end_time":"2022-01-08T11:23:47.678447","exception":false,"start_time":"2022-01-08T11:23:47.373434","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:28.928340Z","iopub.execute_input":"2022-02-06T11:54:28.928551Z","iopub.status.idle":"2022-02-06T11:54:29.209016Z","shell.execute_reply.started":"2022-02-06T11:54:28.928526Z","shell.execute_reply":"2022-02-06T11:54:29.208159Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"I am also interested in the relative positions of discourse types with the essays. Below you can see the distributions of the discourse types of the first and last discourses identified.","metadata":{"papermill":{"duration":0.039472,"end_time":"2022-01-08T11:23:47.756978","exception":false,"start_time":"2022-01-08T11:23:47.717506","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_first = train.drop_duplicates(subset = \"id\", keep = \"first\").discourse_type.value_counts().rename_axis('discourse_type').reset_index(name='counts_first')\ntrain_first['percent_first'] = round((train_first['counts_first']/train.id.nunique()),2)\ntrain_last = train.drop_duplicates(subset = \"id\", keep = \"last\").discourse_type.value_counts().rename_axis('discourse_type').reset_index(name='counts_last')\ntrain_last['percent_last'] = round((train_last['counts_last']/train.id.nunique()),2)\ntrain_first_last = train_first.merge(train_last, on = \"discourse_type\", how = \"left\")\ntrain_first_last","metadata":{"papermill":{"duration":0.125951,"end_time":"2022-01-08T11:23:47.921521","exception":false,"start_time":"2022-01-08T11:23:47.79557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:29.210370Z","iopub.execute_input":"2022-02-06T11:54:29.211128Z","iopub.status.idle":"2022-02-06T11:54:29.311715Z","shell.execute_reply.started":"2022-02-06T11:54:29.211063Z","shell.execute_reply":"2022-02-06T11:54:29.311133Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"We also know that a Lead is missing in around 40% of the essays. Below you can see that if there is a Lead, it's almost always the first discourse identified in an essay (Lead 2 is very rare anyway).","metadata":{"papermill":{"duration":0.03917,"end_time":"2022-01-08T11:23:47.999551","exception":false,"start_time":"2022-01-08T11:23:47.960381","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train['discourse_nr'] = 1\ncounter = 1\n\nfor i in tqdm(range(1, len(train))):\n    if train.loc[i, 'id'] == train.loc[i-1, 'id']:\n        counter += 1\n        train.loc[i, 'discourse_nr'] = counter\n    else:\n        counter = 1\n        train.loc[i, 'discourse_nr'] = counter\n\n#if you are interested in other discourse_types you can add them to the list in df.query\ntrain.query('discourse_type in [\"Lead\"]').groupby('discourse_type_num')['discourse_nr'].value_counts().to_frame('occurences')","metadata":{"papermill":{"duration":94.387974,"end_time":"2022-01-08T11:25:22.426803","exception":false,"start_time":"2022-01-08T11:23:48.038829","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:54:29.312648Z","iopub.execute_input":"2022-02-06T11:54:29.313293Z","iopub.status.idle":"2022-02-06T11:56:02.814971Z","shell.execute_reply.started":"2022-02-06T11:54:29.313263Z","shell.execute_reply":"2022-02-06T11:56:02.814103Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Investigation the gaps between Annotations (text not used as discourse_text)","metadata":{"papermill":{"duration":0.038435,"end_time":"2022-01-08T11:25:22.504362","exception":false,"start_time":"2022-01-08T11:25:22.465927","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Just taking the last discourse_end in train is not entirely correct as a last piece of text may not have been used as a discourse. Therefore, I will go through the essays to find the real ends. Eh....until I remebered that Rob Mulla already did that in the excellent EDA (https://www.kaggle.com/robikscube/student-writing-competition-twitch) ;-). Please upvote his notebook!","metadata":{"papermill":{"duration":0.038599,"end_time":"2022-01-08T11:25:22.581585","exception":false,"start_time":"2022-01-08T11:25:22.542986","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# this code chunk is copied from Rob Mulla\nlen_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)","metadata":{"papermill":{"duration":49.317115,"end_time":"2022-01-08T11:26:11.937506","exception":false,"start_time":"2022-01-08T11:25:22.620391","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:56:02.816274Z","iopub.execute_input":"2022-02-06T11:56:02.816501Z","iopub.status.idle":"2022-02-06T11:56:59.099052Z","shell.execute_reply.started":"2022-02-06T11:56:02.816472Z","shell.execute_reply":"2022-02-06T11:56:59.098218Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"When comparing the discourse_end of the last discourse in each essay, we see that the discourse_end is sometimes larger than the essay_len. This cannot be right, but I will assume that those are last pieces of text in the essay indeed.","metadata":{"papermill":{"duration":0.040263,"end_time":"2022-01-08T11:26:12.017303","exception":false,"start_time":"2022-01-08T11:26:11.97704","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#initialize column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n        #minus 2 as the previous end is always -1 and the previous start always +1\n    #gap if the first discourse of an new essay does not start at 0\n    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n\n\n #is there any text after the last discourse of an essay?\nlast_ones = train.drop_duplicates(subset=\"id\", keep='last')\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id', 'discourse_id', 'gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")","metadata":{"papermill":{"duration":28.838778,"end_time":"2022-01-08T11:26:40.896794","exception":false,"start_time":"2022-01-08T11:26:12.058016","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:56:59.100701Z","iopub.execute_input":"2022-02-06T11:56:59.101029Z","iopub.status.idle":"2022-02-06T11:57:31.337588Z","shell.execute_reply.started":"2022-02-06T11:56:59.100988Z","shell.execute_reply":"2022-02-06T11:57:31.336676Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#display an example\ncols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\ntrain[cols_to_display].query('id == \"AFEC37C2D43F\"')","metadata":{"papermill":{"duration":0.099385,"end_time":"2022-01-08T11:26:41.036005","exception":false,"start_time":"2022-01-08T11:26:40.93662","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.338990Z","iopub.execute_input":"2022-02-06T11:57:31.339246Z","iopub.status.idle":"2022-02-06T11:57:31.398831Z","shell.execute_reply.started":"2022-02-06T11:57:31.339217Z","shell.execute_reply":"2022-02-06T11:57:31.398276Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#how many pieces of tekst are not used as discourses?\nprint(f\"Besides the {len(train)} discourse texts, there are {len(train.query('gap_length.notna()', engine='python'))+ len(train.query('gap_end_length.notna()', engine='python'))} pieces of text not classified.\")","metadata":{"papermill":{"duration":0.06497,"end_time":"2022-01-08T11:26:41.142288","exception":false,"start_time":"2022-01-08T11:26:41.077318","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.399848Z","iopub.execute_input":"2022-02-06T11:57:31.400165Z","iopub.status.idle":"2022-02-06T11:57:31.420141Z","shell.execute_reply.started":"2022-02-06T11:57:31.400138Z","shell.execute_reply":"2022-02-06T11:57:31.419307Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Although the gaps in the example above are small, we do have huge gaps in a number of essays.","metadata":{"papermill":{"duration":0.040887,"end_time":"2022-01-08T11:26:41.224386","exception":false,"start_time":"2022-01-08T11:26:41.183499","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.sort_values(by = \"gap_length\", ascending = False)[cols_to_display].head()","metadata":{"papermill":{"duration":0.091345,"end_time":"2022-01-08T11:26:41.356373","exception":false,"start_time":"2022-01-08T11:26:41.265028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.421219Z","iopub.execute_input":"2022-02-06T11:57:31.421831Z","iopub.status.idle":"2022-02-06T11:57:31.465305Z","shell.execute_reply.started":"2022-02-06T11:57:31.421798Z","shell.execute_reply":"2022-02-06T11:57:31.464726Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train.sort_values(by = \"gap_end_length\", ascending = False)[cols_to_display].head()","metadata":{"papermill":{"duration":0.082868,"end_time":"2022-01-08T11:26:41.48068","exception":false,"start_time":"2022-01-08T11:26:41.397812","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.466497Z","iopub.execute_input":"2022-02-06T11:57:31.466879Z","iopub.status.idle":"2022-02-06T11:57:31.505590Z","shell.execute_reply.started":"2022-02-06T11:57:31.466849Z","shell.execute_reply":"2022-02-06T11:57:31.504807Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Below, you can see a histogram of the length of all gaps with the outliers taken out (all gaps longer than 300 characters).","metadata":{"papermill":{"duration":0.042165,"end_time":"2022-01-08T11:26:41.565059","exception":false,"start_time":"2022-01-08T11:26:41.522894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"all_gaps = (train.gap_length[~train.gap_length.isna()]).append((train.gap_end_length[~train.gap_end_length.isna()]), ignore_index= True)\n#filter outliers\nall_gaps = all_gaps[all_gaps<300]\nfig = plt.figure(figsize=(12,6))\nall_gaps.plot.hist(bins=100)\nplt.title(\"Histogram of gap length (gaps up to 300 characters only)\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Length of gaps in characters\")\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.416704,"end_time":"2022-01-08T11:26:42.023539","exception":false,"start_time":"2022-01-08T11:26:41.606835","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.506784Z","iopub.execute_input":"2022-02-06T11:57:31.507001Z","iopub.status.idle":"2022-02-06T11:57:31.905764Z","shell.execute_reply.started":"2022-02-06T11:57:31.506974Z","shell.execute_reply":"2022-02-06T11:57:31.905136Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Are there many really bad essays (large percentage of text not classified)?\nYes, we do have those. Some have around 90% of text not classified as one of the discourse types.\n\nRegarding the one with gap_end_length 7348: I found out that this student just copied and pasted the same texts multiple times in his/her essay. See discussion topic: https://www.kaggle.com/c/feedback-prize-2021/discussion/298193","metadata":{"papermill":{"duration":0.042227,"end_time":"2022-01-08T11:26:42.108297","exception":false,"start_time":"2022-01-08T11:26:42.06607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"total_gaps = train.groupby('id').agg({'essay_len': 'first',\\\n                                               'gap_length': 'sum',\\\n                                               'gap_end_length': 'sum'})\ntotal_gaps['perc_not_classified'] = round(((total_gaps.gap_length + total_gaps.gap_end_length)/total_gaps.essay_len),2)\n\ntotal_gaps.sort_values(by = 'perc_not_classified', ascending = False).head()","metadata":{"papermill":{"duration":0.096759,"end_time":"2022-01-08T11:26:42.247903","exception":false,"start_time":"2022-01-08T11:26:42.151144","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.906803Z","iopub.execute_input":"2022-02-06T11:57:31.907506Z","iopub.status.idle":"2022-02-06T11:57:31.970135Z","shell.execute_reply.started":"2022-02-06T11:57:31.907469Z","shell.execute_reply":"2022-02-06T11:57:31.969410Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Color printing essays including the gaps\n\nI saw  a very pretty way to do this in the Notebook made by Sanskar Hasija (https://www.kaggle.com/odins0n/feedback-prize-eda). The code is nice but did not print the gaps yet. Below, I make a function that adds all gaps in an essay as rows with discourse type \"Nothing\".","metadata":{"papermill":{"duration":0.042576,"end_time":"2022-01-08T11:26:42.333079","exception":false,"start_time":"2022-01-08T11:26:42.290503","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)","metadata":{"papermill":{"duration":0.056084,"end_time":"2022-01-08T11:26:42.434679","exception":false,"start_time":"2022-01-08T11:26:42.378595","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.971377Z","iopub.execute_input":"2022-02-06T11:57:31.971609Z","iopub.status.idle":"2022-02-06T11:57:31.983976Z","shell.execute_reply.started":"2022-02-06T11:57:31.971581Z","shell.execute_reply":"2022-02-06T11:57:31.982927Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"add_gap_rows(\"129497C3E0FC\")","metadata":{"papermill":{"duration":0.075007,"end_time":"2022-01-08T11:26:42.553614","exception":false,"start_time":"2022-01-08T11:26:42.478607","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:31.985457Z","iopub.execute_input":"2022-02-06T11:57:31.985834Z","iopub.status.idle":"2022-02-06T11:57:32.032497Z","shell.execute_reply.started":"2022-02-06T11:57:31.985789Z","shell.execute_reply":"2022-02-06T11:57:32.031683Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"This enables me to make a function that uses the code made by Sanskar Hasija to color print an essay including the gaps.","metadata":{"papermill":{"duration":0.043313,"end_time":"2022-01-08T11:26:42.640038","exception":false,"start_time":"2022-01-08T11:26:42.596725","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def print_colored_essay(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https://www.kaggle.com/odins0n/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"../input/feedback-prize-2021/train/\" + essay + \".txt\"\n\n    ents = []\n    for i, row in df_essay.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(essay_file, 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n    }\n\n    colors = {'Lead': '#EE11D0','Position': '#AB4DE1','Claim': '#1EDE71','Evidence': '#33FAFA','Counterclaim': '#4253C1','Concluding Statement': 'yellow','Rebuttal': 'red'}\n    options = {\"ents\": df_essay.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","metadata":{"papermill":{"duration":0.052949,"end_time":"2022-01-08T11:26:42.736494","exception":false,"start_time":"2022-01-08T11:26:42.683545","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:32.033803Z","iopub.execute_input":"2022-02-06T11:57:32.034638Z","iopub.status.idle":"2022-02-06T11:57:32.044089Z","shell.execute_reply.started":"2022-02-06T11:57:32.034575Z","shell.execute_reply":"2022-02-06T11:57:32.043376Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print_colored_essay(\"7330313ED3F0\")","metadata":{"papermill":{"duration":0.067074,"end_time":"2022-01-08T11:26:42.848481","exception":false,"start_time":"2022-01-08T11:26:42.781407","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:32.048964Z","iopub.execute_input":"2022-02-06T11:57:32.049172Z","iopub.status.idle":"2022-02-06T11:57:32.077977Z","shell.execute_reply.started":"2022-02-06T11:57:32.049149Z","shell.execute_reply":"2022-02-06T11:57:32.077455Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Most used words per Discourse Type\n\nInitially, I did a manual effort to find out which single words were used most often.I took out stopwords, converted all text to lowercase, but left in the punctuation. I also took out some extra words that were all over the place in the figures for each discourse_type. After this effort, I was not sure how useful this is. One thing to notice is that \"however,\" is used a lot in Rebuttal.\n\nLater on, I decided that making one function for all n_grams was the way to go. If you are still interested in my manual effort for the single words, you can unhide the code in the cell below.","metadata":{"papermill":{"duration":0.043495,"end_time":"2022-01-08T11:26:42.937469","exception":false,"start_time":"2022-01-08T11:26:42.893974","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train['discourse_text'] = train['discourse_text'].str.lower()\n\n#get stopwords from nltk library\nstop_english = stopwords.words(\"english\")\nother_words_to_take_out = ['school', 'students', 'people', 'would', 'could', 'many']\nstop_english.extend(other_words_to_take_out)\n\n#put dataframe of Top-10 words in dict for all discourse types\ncounts_dict = {}\nfor dt in train['discourse_type'].unique():\n    df = train.query('discourse_type == @dt')\n    text = df.discourse_text.apply(lambda x: x.split()).tolist()\n    text = [item for elem in text for item in elem]\n    df1 = pd.Series(text).value_counts().to_frame().reset_index()\n    df1.columns = ['Word', 'Frequency']\n    df1 = df1[~df1.Word.isin(stop_english)].head(10)\n    df1 = df1.set_index(\"Word\").sort_values(by = \"Frequency\", ascending = True)\n    counts_dict[dt] = df1\n\nplt.figure(figsize=(15, 12))\nplt.subplots_adjust(hspace=0.5)\n\nkeys = list(counts_dict.keys())\n\nfor n, key in enumerate(keys):\n    ax = plt.subplot(4, 2, n + 1)\n    ax.set_title(f\"Most used words in {key}\")\n    counts_dict[keys[n]].plot(ax=ax, kind = 'barh')\n    plt.ylabel(\"\")\n\nplt.show()","metadata":{"_kg_hide-input":true,"papermill":{"duration":4.325939,"end_time":"2022-01-08T11:26:47.307276","exception":false,"start_time":"2022-01-08T11:26:42.981337","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:32.079009Z","iopub.execute_input":"2022-02-06T11:57:32.079250Z","iopub.status.idle":"2022-02-06T11:57:37.352045Z","shell.execute_reply.started":"2022-02-06T11:57:32.079223Z","shell.execute_reply":"2022-02-06T11:57:37.351223Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Making n_grams for each discourse type\n\nAfter the manual effort above, I was not fully pleased with the result and decided that I wanted to make a function to compose Top-10 n_grams per discount type by using CountVectorizer(). This function should also work for the single words (just run it with n_grams =1).","metadata":{"papermill":{"duration":0.050017,"end_time":"2022-01-08T11:26:47.405352","exception":false,"start_time":"2022-01-08T11:26:47.355335","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_n_grams(n_grams, top_n = 10):\n    df_words = pd.DataFrame()\n    for dt in tqdm(train['discourse_type'].unique()):\n        df = train.query('discourse_type == @dt')\n        texts = df['discourse_text'].tolist()\n        vec = CountVectorizer(lowercase = True, stop_words = 'english',\\\n                              ngram_range=(n_grams, n_grams)).fit(texts)\n        bag_of_words = vec.transform(texts)\n        sum_words = bag_of_words.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n        cvec_df = pd.DataFrame.from_records(words_freq,\\\n                                            columns= ['words', 'counts']).sort_values(by=\"counts\", ascending=False)\n        cvec_df.insert(0, \"Discourse_type\", dt)\n        cvec_df = cvec_df.iloc[:top_n,:]\n        df_words = df_words.append(cvec_df)\n    return df_words","metadata":{"papermill":{"duration":0.061356,"end_time":"2022-01-08T11:26:47.517838","exception":false,"start_time":"2022-01-08T11:26:47.456482","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:37.353375Z","iopub.execute_input":"2022-02-06T11:57:37.353608Z","iopub.status.idle":"2022-02-06T11:57:37.362253Z","shell.execute_reply.started":"2022-02-06T11:57:37.353579Z","shell.execute_reply":"2022-02-06T11:57:37.361493Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"This function return one dataframe with 70 rows (the top 10 most used n-grams for each discourse type).","metadata":{"papermill":{"duration":0.049424,"end_time":"2022-01-08T11:26:47.615998","exception":false,"start_time":"2022-01-08T11:26:47.566574","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bigrams = get_n_grams(n_grams = 2, top_n=10)\nbigrams.head()","metadata":{"papermill":{"duration":17.554968,"end_time":"2022-01-08T11:27:05.220013","exception":false,"start_time":"2022-01-08T11:26:47.665045","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:57:37.363300Z","iopub.execute_input":"2022-02-06T11:57:37.363548Z","iopub.status.idle":"2022-02-06T11:58:01.056457Z","shell.execute_reply.started":"2022-02-06T11:57:37.363518Z","shell.execute_reply":"2022-02-06T11:58:01.055619Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Below, I have also made a function that prints the results in this dataframe as subplots.","metadata":{"papermill":{"duration":0.049215,"end_time":"2022-01-08T11:27:05.319672","exception":false,"start_time":"2022-01-08T11:27:05.270457","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_ngram(df, type = \"bigrams\"):\n    plt.figure(figsize=(15, 12))\n    plt.subplots_adjust(hspace=0.5)\n\n    for n, dt in enumerate(df.Discourse_type.unique()):\n        ax = plt.subplot(4, 2, n + 1)\n        ax.set_title(f\"Most used {type} in {dt}\")\n        data = df.query('Discourse_type == @dt')[['words', 'counts']].set_index(\"words\").sort_values(by = \"counts\", ascending = True)\n        data.plot(ax=ax, kind = 'barh')\n        plt.ylabel(\"\")\n    plt.tight_layout()\n    plt.show()\n    \nplot_ngram(bigrams)","metadata":{"papermill":{"duration":1.579663,"end_time":"2022-01-08T11:27:06.94877","exception":false,"start_time":"2022-01-08T11:27:05.369107","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:58:01.057552Z","iopub.execute_input":"2022-02-06T11:58:01.057842Z","iopub.status.idle":"2022-02-06T11:58:02.742548Z","shell.execute_reply.started":"2022-02-06T11:58:01.057813Z","shell.execute_reply":"2022-02-06T11:58:02.741712Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Below, I am also plotting the trigrams using both functions in one go.","metadata":{"execution":{"iopub.execute_input":"2021-12-28T14:49:19.545062Z","iopub.status.busy":"2021-12-28T14:49:19.543924Z","iopub.status.idle":"2021-12-28T14:49:21.441599Z","shell.execute_reply":"2021-12-28T14:49:21.4376Z","shell.execute_reply.started":"2021-12-28T14:49:19.545018Z"},"papermill":{"duration":0.055068,"end_time":"2022-01-08T11:27:07.059266","exception":false,"start_time":"2022-01-08T11:27:07.004198","status":"completed"},"tags":[]}},{"cell_type":"code","source":"trigrams = get_n_grams(n_grams = 3, top_n=10)\nplot_ngram(trigrams, type = \"trigrams\")","metadata":{"papermill":{"duration":23.826884,"end_time":"2022-01-08T11:27:30.941441","exception":false,"start_time":"2022-01-08T11:27:07.114557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:58:02.743964Z","iopub.execute_input":"2022-02-06T11:58:02.744947Z","iopub.status.idle":"2022-02-06T11:58:33.744704Z","shell.execute_reply.started":"2022-02-06T11:58:02.744901Z","shell.execute_reply":"2022-02-06T11:58:33.743766Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# NER Introduction\nFirst I am ging to convert the words in all train discourses into NER labels. I am basically using the loop found in Chris Deotte's great notebook (https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615 Please upvote his notebook!), but tried to make it a little easier to understand. I am also using df.loc instead of df.iterrows.\n\nFirst, we have to make a dataframe with all full texts of the essays in a dataframe.","metadata":{"papermill":{"duration":0.061827,"end_time":"2022-01-08T11:27:31.067156","exception":false,"start_time":"2022-01-08T11:27:31.005329","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{"papermill":{"duration":7.370405,"end_time":"2022-01-08T11:27:38.498413","exception":false,"start_time":"2022-01-08T11:27:31.128008","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:58:33.746249Z","iopub.execute_input":"2022-02-06T11:58:33.746901Z","iopub.status.idle":"2022-02-06T11:58:40.841114Z","shell.execute_reply.started":"2022-02-06T11:58:33.746853Z","shell.execute_reply":"2022-02-06T11:58:40.840238Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#please be aware that since I used df.loc, which is easier to read, you have to be careful with resetting the index if necessary\n\nall_entities = []\n#loop over dataframe with all full texts\nfor i in tqdm(range(len(train_text_df))):\n    total = len(train_text_df.loc[i, 'text'].split())\n    #now a list with length the total number of words in an essay is initialised with all values being \"O\"\n    entities = [\"O\"]*total\n    #now loop over dataframe with all discourses of this particular essay\n    discourse_id = train_text_df.loc[i, 'id']\n    train_df_id = train.query('id == @discourse_id').reset_index(drop=True)\n    for j in range(len(train_df_id)):\n        discourse = train_df_id.loc[j, 'discourse_type']\n        #make a list with the position numbers in predictionstring converted into integer\n        list_ix = [int(x) for x in train_df_id.loc[j, 'predictionstring'].split(' ')]\n        #now the entities lists gets filled with \"real values\"\n        #the first word of each discourse gets prefix \"Beginning\"\n        entities[list_ix[0]] = f\"B-{discourse}\"\n        #the other ones get prefix I\n        for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n    all_entities.append(entities)\n    \n    \ntrain_text_df['entities'] = all_entities","metadata":{"papermill":{"duration":102.331504,"end_time":"2022-01-08T11:29:20.892921","exception":false,"start_time":"2022-01-08T11:27:38.561417","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T11:58:40.842390Z","iopub.execute_input":"2022-02-06T11:58:40.842599Z","iopub.status.idle":"2022-02-06T12:00:57.235004Z","shell.execute_reply.started":"2022-02-06T11:58:40.842566Z","shell.execute_reply":"2022-02-06T12:00:57.234121Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_text_df.head()","metadata":{"papermill":{"duration":0.080668,"end_time":"2022-01-08T11:29:21.036368","exception":false,"start_time":"2022-01-08T11:29:20.9557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T12:00:57.236622Z","iopub.execute_input":"2022-02-06T12:00:57.237110Z","iopub.status.idle":"2022-02-06T12:00:57.256169Z","shell.execute_reply.started":"2022-02-06T12:00:57.237046Z","shell.execute_reply":"2022-02-06T12:00:57.255377Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**To be continued. Please stay tuned!**","metadata":{"papermill":{"duration":0.063837,"end_time":"2022-01-08T11:29:21.164357","exception":false,"start_time":"2022-01-08T11:29:21.10052","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.loc[train['discourse_type']=='Evidence',['id','discourse_type_num','discourse_text','predictionstring']].sort_values('discourse_type_num')","metadata":{"papermill":{"duration":0.06357,"end_time":"2022-01-08T11:29:21.29095","exception":false,"start_time":"2022-01-08T11:29:21.22738","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T12:17:38.982030Z","iopub.execute_input":"2022-02-06T12:17:38.982475Z","iopub.status.idle":"2022-02-06T12:17:39.093348Z","shell.execute_reply.started":"2022-02-06T12:17:38.982441Z","shell.execute_reply":"2022-02-06T12:17:39.092364Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print_colored_essay(\"D7982DFB10E1\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:18:14.113533Z","iopub.execute_input":"2022-02-06T12:18:14.113823Z","iopub.status.idle":"2022-02-06T12:18:14.152026Z","shell.execute_reply.started":"2022-02-06T12:18:14.113794Z","shell.execute_reply":"2022-02-06T12:18:14.151181Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print_colored_essay(\"2714214F7D9E\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:19:10.131914Z","iopub.execute_input":"2022-02-06T12:19:10.132273Z","iopub.status.idle":"2022-02-06T12:19:10.171736Z","shell.execute_reply.started":"2022-02-06T12:19:10.132240Z","shell.execute_reply":"2022-02-06T12:19:10.171120Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"train.loc[train['discourse_type']=='Claim',['id','discourse_type_num','discourse_text','predictionstring']].sort_values('discourse_type_num')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:19:51.972640Z","iopub.execute_input":"2022-02-06T12:19:51.973332Z","iopub.status.idle":"2022-02-06T12:19:52.087864Z","shell.execute_reply.started":"2022-02-06T12:19:51.973297Z","shell.execute_reply":"2022-02-06T12:19:52.087182Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"print_colored_essay(\"8AE9824CC802\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:20:34.322403Z","iopub.execute_input":"2022-02-06T12:20:34.322843Z","iopub.status.idle":"2022-02-06T12:20:34.353838Z","shell.execute_reply.started":"2022-02-06T12:20:34.322811Z","shell.execute_reply":"2022-02-06T12:20:34.352984Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}