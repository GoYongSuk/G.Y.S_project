{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf1fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578bb0e",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee481e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 0s 0us/step\n",
      "2654208/2638744 [==============================] - 0s 0us/step\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55297c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 118964\n",
      ">> Tom said he didn't know why Mary wanted to break up with him.\tTom dijo que él no sabía por qué María quería terminar con él.\n",
      ">> Tom is thinking it over.\tTom lo está reflexionando.\n",
      ">> This medicine may cure him.\tEsta medicina podría curarlo.\n",
      ">> That shirt looks good on you.\tEsa camisa te queda bien.\n",
      ">> I already know what I'm going to do.\tYa sé lo que haré.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.dirname(zip_path)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    spa_eng_sentences = f.read().splitlines()\n",
    "\n",
    "spa_eng_sentences = list(set(spa_eng_sentences)) \n",
    "total_sentence_count = len(spa_eng_sentences)\n",
    "print(\"Example:\", total_sentence_count)\n",
    "\n",
    "for sen in spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf849f",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04aab644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9252f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305e2318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  594\n",
      "\n",
      "\n",
      "Train Example: 118370\n",
      ">> tom said he didn't know why mary wanted to break up with him.\ttom dijo que él no sabía por qué maría quería terminar con él.\n",
      ">> tom is thinking it over.\ttom lo está reflexionando.\n",
      ">> this medicine may cure him.\testa medicina podría curarlo.\n",
      ">> that shirt looks good on you.\tesa camisa te queda bien.\n",
      ">> i already know what i'm going to do.\tya sé lo que haré.\n",
      "\n",
      "\n",
      "Test Example: 594\n",
      ">> you've always helped me.\tsiempre me has ayudado.\n",
      ">> we were framed.\tfuimos enmarcados.\n",
      ">> how many friends do you have on facebook?\t¿cuántos amigos tienes en facebook?\n",
      ">> tom needs a job.\ttom necesita un trabajo.\n",
      ">> if i tell my mother, she'll worry, so i don't think i'll tell her.\tsi se lo digo a mi madre, se preocupará, así que no creo que vaya a decírselo.\n"
     ]
    }
   ],
   "source": [
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_spa_eng_sentences = spa_eng_sentences[:-test_sentence_count]\n",
    "test_spa_eng_sentences = spa_eng_sentences[-test_sentence_count:]\n",
    "print(\"Train Example:\", len(train_spa_eng_sentences))\n",
    "for sen in train_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test Example:\", len(test_spa_eng_sentences))\n",
    "for sen in test_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351c7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "        spa_sentences.append(spa_sentence)\n",
    "        eng_sentences.append(eng_sentence)\n",
    "    return eng_sentences, spa_sentences\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b544ac2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1542310720c04178a78efb61c976b1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118370\n",
      "tom said he didn't know why mary wanted to break up with him.\n",
      "\n",
      "\n",
      "118370\n",
      "tom dijo que él no sabía por qué maría quería terminar con él.\n"
     ]
    }
   ],
   "source": [
    "train_eng_sentences, train_spa_sentences = split_spa_eng_sentences(train_spa_eng_sentences)\n",
    "print(len(train_eng_sentences))\n",
    "print(train_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(train_spa_sentences))\n",
    "print(train_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0504c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7296c80a0c4ee78225a05da590bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "you've always helped me.\n",
      "\n",
      "\n",
      "594\n",
      "siempre me has ayudado.\n"
     ]
    }
   ],
   "source": [
    "test_eng_sentences, test_spa_sentences = split_spa_eng_sentences(test_spa_eng_sentences)\n",
    "print(len(test_eng_sentences))\n",
    "print(test_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(test_spa_sentences))\n",
    "print(test_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98ef35",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a38cc82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db967d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=././spa-eng_corpus.txt --model_prefix=spa-eng_spm --vocab_size=20000--pad_id==0 --bos_id=1 --eos_id=2 --unk_id=3"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ././spa-eng_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spa-eng_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ././spa-eng_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 236740 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=7877987\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9547% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=43\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999547\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 236740 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 110181 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 236740\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 62421\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 62421 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35517 obj=10.7519 num_tokens=118507 num_tokens/piece=3.33663\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=28148 obj=8.52287 num_tokens=118997 num_tokens/piece=4.22755\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21998 obj=8.47728 num_tokens=125753 num_tokens/piece=5.71657\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21984 obj=8.46314 num_tokens=125816 num_tokens/piece=5.72307\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spa-eng_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spa-eng_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(train_eng_sentences + train_spa_sentences, VOCAB_SIZE, 'spa-eng')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "936cb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b171a19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f220b34e1a4f0c86895e25e6a56e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6da0de21724fef86029659915d39bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_corpus = make_corpus(train_eng_sentences, tokenizer)\n",
    "spa_corpus = make_corpus(train_spa_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1f988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom said he didn't know why mary wanted to break up with him.\n",
      "[1, 5, 227, 23, 61, 35, 7, 17, 74, 146, 30, 306, 10, 1320, 107, 66, 78, 0, 2]\n",
      "\n",
      "\n",
      "tom dijo que él no sabía por qué maría quería terminar con él.\n",
      "[1, 5, 206, 15, 36, 14, 409, 43, 88, 282, 283, 1048, 50, 36, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_eng_sentences[0])\n",
    "print(eng_corpus[0])\n",
    "print('\\n')\n",
    "print(train_spa_sentences[0])\n",
    "print(spa_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4720e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(eng_corpus, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(spa_corpus, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0823a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cb0b2",
   "metadata": {},
   "source": [
    "# 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f25111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52f157",
   "metadata": {},
   "source": [
    "# mask 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26beb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7c7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f021503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e46f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d4859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bb46e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3050629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97c66511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39290818",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b61983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1056d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a2c2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4eab54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96b933c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99aa19beda5a49c2bef30e202f231c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354d9b8d00b54ce999f0a02f14260257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821241050f0e4daf9f13adfbb0a11931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc43c6e",
   "metadata": {},
   "source": [
    "# 번역 성능 측정하기 BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db7494",
   "metadata": {},
   "source": [
    "### NLTK를 활용한 BLEU Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60a7ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# 아래 두 문장을 바꿔가며 테스트 해보세요\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5e2a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869cb42",
   "metadata": {},
   "source": [
    "### SmoothingFunction()으로 BLEU Score 보정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b9ee6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1bbb0",
   "metadata": {},
   "source": [
    "# 트랜스포머 모델의 번역 성능 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7886e4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  \n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return result\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da821e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e0caac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  you've always helped me.\n",
      "Model Prediction:  ['siempre', 'me', 'has', 'ayudado', 'conmigo?']\n",
      "Real:  ['siempre', 'me', 'has', 'ayudado.']\n",
      "Score: 0.265915\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26591479484724945"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스를 바꿔가며 테스트해 보세요\n",
    "test_idx = 0\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 test_eng_sentences[test_idx], \n",
    "                 test_spa_sentences[test_idx], \n",
    "                 tokenizer, \n",
    "                 tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550d089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a33350e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daf6069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5283ddb9ae49afbd7883f265228918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 594\n",
      "Total Score: 0.10848220959638938\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, test_eng_sentences, test_spa_sentences, tokenizer, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d9a96",
   "metadata": {},
   "source": [
    "# 번역 성능 측정하기 (2) Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2465d80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "855730ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f96450",
   "metadata": {},
   "source": [
    "### Beam Search Decoder 작성 및 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecf22340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "    \n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f458ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    \n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.eos_id())\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a74c9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e23dbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bef4297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['él', 'huele', 'como', 'una', 'cabra.']\n",
      "Candidate: ['él', 'huele', 'como', 'una', 'cabra!']\n",
      "BLEU: 0.668740304976422\n",
      "Reference: ['él', 'huele', 'como', 'una', 'cabra.']\n",
      "Candidate: ['él', 'huele', 'como', 'una', 'cabra', 'en']\n",
      "BLEU: 0.5081327481546147\n",
      "Reference: ['él', 'huele', 'como', 'una', 'cabra.']\n",
      "Candidate: ['él', 'huele', 'como', 'una', 'cabra', 'de']\n",
      "BLEU: 0.5081327481546147\n",
      "Reference: ['él', 'huele', 'como', 'una', 'cabra.']\n",
      "Candidate: ['él', 'huele', 'a', 'una', 'cabra!']\n",
      "BLEU: 0.12574334296829354\n",
      "Reference: ['él', 'huele', 'como', 'una', 'cabra.']\n",
      "Candidate: ['él', 'huele', 'como', 'una', 'cabra', 'a']\n",
      "BLEU: 0.5081327481546147\n",
      "0.46377637848171194\n"
     ]
    }
   ],
   "source": [
    "# 인덱스를 바꿔가며 확인해 보세요\n",
    "test_idx = 1\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(test_eng_sentences[test_idx],\n",
    "                    MAX_LEN,\n",
    "                    MAX_LEN,\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(test_spa_sentences[test_idx], ids, tokenizer)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f6e50",
   "metadata": {},
   "source": [
    "# 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96970576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a63d0dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462778806686401),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.52181077003479),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.48399588465690613),\n",
       " ('peanut', 0.48062023520469666),\n",
       " ('potato', 0.48061180114746094)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d12d4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? all you needs is attention . \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40ed33",
   "metadata": {},
   "source": [
    "# Lexical Substitution 구현하기\n",
    "\n",
    " Embedding 유사도를 기반으로 Augmentation 하여 반환하는 lexical_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6cc5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40b3dc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9790975d6c084788afff457f86c84f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"you've always helping me. \", \"you've always helped me.\", 'he smells such a goat. ', 'he smells like a goat.', \"perhaps that's true.\", 'this melody is unfamiliar to many japanese. ', 'this melody is familiar to many japanese.', \"i'm sure tom wouldn't want you to catch cold.\", 'there is nothing intriguing in the newspaper today. ', 'there is nothing interesting in the newspaper today.']\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []\n",
    "\n",
    "for old_src in tqdm(test_eng_sentences):\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "    if new_src is not None: \n",
    "        new_corpus.append(new_src)\n",
    "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
